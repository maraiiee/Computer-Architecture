The Von Neumann architecture, introduced in 1945, defines a computer system where data and instructions share the same memory and bus, enabling stored-program functionality. It features a processing unit (with ALU and control unit), memory, I/O systems, and the ability to execute sequential instructions. While simpler than the Harvard architecture, it suffers from the "Von Neumann bottleneck", a performance limitation due to a shared data/instruction pathway. The design allows self-modifying code and forms the foundation of most modern computers, though its lack of built-in security has made it vulnerable to code injection attacks.
ScienceDirect. (n.d.). Von Neumann architecture. In Topics in Computer Science. Elsevier. Retrieved from https://www.sciencedirect.com/topics/computer-science/von-neumann-architecture

Computers simulate classical physics using quantum principles and are built from transistors made of doped semiconductors. Using transistors, we create logic gates and memory types like DRAM and SRAM. The manufacturing process, photolithography, builds chips layer by layer. Moore’s Law continues to drive growth in transistor counts, leading to multi-core processors. However, while multi-core chips enable parallel processing, leveraging their full potential requires rethinking algorithms, languages, and programming approaches to scale with increasing core counts.
University of Virginia. (n.d.). CS 213: Introduction to Computer Systems – From Physics to Logic. Retrieved from https://users.cs.northwestern.edu/~pdinda/icsclass/doc/physics.pdf

